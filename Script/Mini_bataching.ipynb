{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mini-batching**:\n",
    "Uno dei più grandi problemi del processare i dati singolarmente, cioè uno per volta è che non possiamo scalare bene in ambito big data. Processando un solo elemento alla volta non possiamo sfruttare i vantaggi della vettorizzazione, cioè tutti quei metodi che abbiamo imparato nella versione batch.\n",
    "\n",
    "Quindi vedendo tutto dall'alto noi `River` utilizza un ciclo `for loop`, che diciamo può soffrire in taluni casi quando sono richieste velocità maggiori... Questo non per dire che `river` è lento, diciamo che invece per processare un elemento alla volta è molto più veloce di altre librerie... quindi possiamo dire che river è veloce in questo contesto anche perchè le altre librerie si basano più su dati batches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nel prossimo esempio vedremo e costruiremo una Pipeline che ci permette di scalare e trainare un regressore logistico\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from river import compose\n",
    "from river import linear_model\n",
    "from river import preprocessing\n",
    "\n",
    "model = compose.Pipeline(\n",
    "    preprocessing.StandardScaler(),\n",
    "    linear_model.LogisticRegression()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://archive.ics.uci.edu/ml/machine-learning-databases/00280/HIGGS.csv.gz (2.62 GB)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Higgs dataset.\n",
       "\n",
       "The data has been produced using Monte Carlo simulations. The first 21 features (columns 2-22)\n",
       "are kinematic properties measured by the particle detectors in the accelerator. The last seven\n",
       "features are functions of the first 21 features; these are high-level features derived by\n",
       "physicists to help discriminate between the two classes.\n",
       "\n",
       "      Name  Higgs                                                                       \n",
       "      Task  Binary classification                                                       \n",
       "   Samples  11,000,000                                                                  \n",
       "  Features  28                                                                          \n",
       "    Sparse  False                                                                       \n",
       "      Path  /home/giodefa/river_data/Higgs/HIGGS.csv.gz                                 \n",
       "       URL  https://archive.ics.uci.edu/ml/machine-learning-databases/00280/HIGGS.csv.gz\n",
       "      Size  2.62 GB                                                                     \n",
       "Downloaded  True                                                                        "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from river import datasets\n",
    "\n",
    "dataset = datasets.Higgs()\n",
    "if not dataset.is_downloaded:\n",
    "    dataset.download()\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "names = [\n",
    "    'target', 'lepton pT', 'lepton eta', 'lepton phi',\n",
    "    'missing energy magnitude', 'missing energy phi',\n",
    "    'jet 1 pt', 'jet 1 eta', 'jet 1 phi', 'jet 1 b-tag',\n",
    "    'jet 2 pt', 'jet 2 eta', 'jet 2 phi', 'jet 2 b-tag',\n",
    "    'jet 3 pt', 'jet 3 eta', 'jet 3 phi', 'jet 3 b-tag',\n",
    "    'jet 4 pt', 'jet 4 eta', 'jet 4 phi', 'jet 4 b-tag',\n",
    "    'm_jj', 'm_jjj', 'm_lv', 'm_jlv', 'm_bb', 'm_wbb', 'm_wwbb'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in pd.read_csv(dataset.path, names=names, chunksize=8096, nrows=3e5):\n",
    "    y = x.pop('target')\n",
    "    y_pred = model.predict_proba_many(x)\n",
    "    model.learn_many(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OneClassSVM\n",
      "MiniBatchClassifier\n",
      "MiniBatchRegressor\n",
      "MiniBatchTransformer\n",
      "SKL2RiverClassifier\n",
      "SKL2RiverRegressor\n",
      "Pipeline\n",
      "Select\n",
      "TransformerProduct\n",
      "TransformerUnion\n",
      "BagOfWords\n",
      "TFIDF\n",
      "LinearRegression\n",
      "LogisticRegression\n",
      "Perceptron\n",
      "OneVsRestClassifier\n",
      "BernoulliNB\n",
      "ComplementNB\n",
      "MultinomialNB\n",
      "MLPRegressor\n",
      "OneHotEncoder\n",
      "StandardScaler\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import inspect\n",
    "\n",
    "def can_mini_batch(obj):\n",
    "    return hasattr(obj, 'learn_many')\n",
    "\n",
    "for module in importlib.import_module('river').__all__:\n",
    "    if module in ['datasets', 'synth']:\n",
    "        continue\n",
    "    for name, obj in inspect.getmembers(importlib.import_module(f'river.{module}'), can_mini_batch):\n",
    "        print(name)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "59f5992ab00c558a84279d33bb134f8aa55f204103f96d48d70e349812082309"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('river')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
