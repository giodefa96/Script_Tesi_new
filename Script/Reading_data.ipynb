{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Bike sharing station information from the city of Toulouse.\n\nThe goal is to predict the number of bikes in 5 different bike stations from the city of\nToulouse.\n\n      Name  Bikes                                                         \n      Task  Regression                                                    \n   Samples  182,470                                                       \n  Features  8                                                             \n    Sparse  False                                                         \n      Path  C:\\Users\\giode\\river_data\\Bikes\\toulouse_bikes.csv            \n       URL  https://maxhalford.github.io/files/datasets/toulouse_bikes.zip\n      Size  12.52 MB                                                      \nDownloaded  True                                                          "
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from river import datasets\n",
    "\n",
    "dataset = datasets.Bikes()\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'moment': datetime.datetime(2016, 4, 1, 0, 0, 7),\n",
       "  'station': 'metro-canal-du-midi',\n",
       "  'clouds': 75,\n",
       "  'description': 'light rain',\n",
       "  'humidity': 81,\n",
       "  'pressure': 1017.0,\n",
       "  'temperature': 6.54,\n",
       "  'wind': 9.3},\n",
       " 1)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = next(iter(dataset))\n",
    "x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'moment': '2016-04-01 00:00:07',\n",
       "  'bikes': '1',\n",
       "  'station': 'metro-canal-du-midi',\n",
       "  'clouds': '75',\n",
       "  'description': 'light rain',\n",
       "  'humidity': '81',\n",
       "  'pressure': '1017.0',\n",
       "  'temperature': '6.54',\n",
       "  'wind': '9.3'},\n",
       " None)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from river import stream\n",
    "\n",
    "X_y = stream.iter_csv(dataset.path)\n",
    "x, y = next(X_y)\n",
    "x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Come vediamo all'interno del nosto dizionario non abbiamo fatto il cast dei numeri ma abbiamo stringhe, infatti se carichiamo attraverso ```stream.inter_csv``` assumiamo che tutto sia a stringa, poi altro problema che possiamo avere è il fatto che il Moment non è in datetime ed infine abbiamo il valore fondamentale il valore bikes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'moment': datetime.datetime(2016, 4, 1, 0, 0, 7),\n",
       "  'station': 'metro-canal-du-midi',\n",
       "  'clouds': 75,\n",
       "  'description': 'light rain',\n",
       "  'humidity': 81,\n",
       "  'pressure': 1017.0,\n",
       "  'temperature': 6.54,\n",
       "  'wind': 9.3},\n",
       " 1)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_y = stream.iter_csv(\n",
    "    dataset.path,\n",
    "    converters={\n",
    "        'bikes': int,\n",
    "        'clouds': int,\n",
    "        'humidity' : int,\n",
    "        'pressure': float,\n",
    "        'temperature': float,\n",
    "        'wind': float\n",
    "    },\n",
    "    parse_dates={'moment': '%Y-%m-%d %H:%M:%S'},\n",
    "    target='bikes'\n",
    ")\n",
    "\n",
    "x, y = next(X_y)\n",
    "x,y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Quindi River non è lo strumento migliore per il preporcessing, quindi magari sarebbe sempre meglio optare per soluzioni differenti, vediamo come anche la posizione potrebbe essere una features, e la descirzione del clima.... ci sono tante cose che possono essere migliorate!\n",
    "Altra soluzione è quella di poter convertire anche altre tipologie di dataset in modo da poter essere lette in formato stream, per esempio abbiamo il modulo ```stream.iter_sklearn_dataset``` che permette di converitre un dataset scikit-learn dataset in uno stream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'age': 0.0380759064334241,\n",
       "  'sex': 0.0506801187398187,\n",
       "  'bmi': 0.0616962065186885,\n",
       "  'bp': 0.0218723549949558,\n",
       "  's1': -0.0442234984244464,\n",
       "  's2': -0.0348207628376986,\n",
       "  's3': -0.0434008456520269,\n",
       "  's4': -0.00259226199818282,\n",
       "  's5': 0.0199084208763183,\n",
       "  's6': -0.0176461251598052},\n",
       " 151.0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "dataset = datasets.load_diabetes()\n",
    "\n",
    "for x, y in stream.iter_sklearn_dataset(dataset):\n",
    "    break\n",
    "\n",
    "x, y \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Dobbiamo fare una piccola zoomata sulla differenza tra **Proactive** learning e **Reactive Learning**, dove nel primo caso noi abbiamo già il nostro dataset e lo iteriamo tramite un ciclo For, quindi chiamiamo noi il tutto.\n",
    "\n",
    "Dall'altro lato in una situazione di proactive learning invece non abbiamo controllo sull'arrivo dei dati, per esempio siamo in un contesto web server dove le web request arrivano in modo albitrario. Ed è qui che River brilla è può far vedere il suo meglio. \n",
    "\n",
    "Possiamo per esempio avere un applicazione che fa uso di Flask, e quindi possiamo definire u a strata al make prediction con river vediamo un esempio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import flask\n",
    "\n",
    "app = flask.Flask(__name__)\n",
    "\n",
    "@app.route('/', methods = ['GET'])\n",
    "def predict():\n",
    "    payload = flask.request.json\n",
    "    river_model = load_model()\n",
    "    return river_model.predict_proba_one(payload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "@app.route('/', methods=['POST'])\n",
    "def learn():\n",
    "    payload = flask.request.json\n",
    "    river_model = load_model()\n",
    "    river_model.learn_one(payload['features'], payload['target'])\n",
    "    return {}, 201"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "59f5992ab00c558a84279d33bb134f8aa55f204103f96d48d70e349812082309"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('river')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}